{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhussn/SecSplitLLM/blob/main/SecSplitLLM/notebooks/gpt-2/Benchmark_GPT2_SST_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0BCHpMtIxJT"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers[torch] accelerate -U\n",
        "!pip install datasets evaluate\n",
        "!pip install trl\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "21XyIM0y3U0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd # Imports the panda library from huggin face to load and visualize the dataset\n",
        "df = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/plain_text/train-00000-of-00001.parquet\")\n",
        "df_sample = df.sample(frac=0.014)\n",
        "print(df_sample)\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_PROJECT\"] = \"gpt2-classification\"\n",
        "os.environ[\"WANDB_WATCH\"] = \"all\"\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Convert the pandas DataFrame to a Dataset\n",
        "dataset = Dataset.from_pandas(df_sample)\n",
        "\n",
        "# Load the GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token # GPT-2 doesn't have a padding token by default\n",
        "\n",
        "# Tokenize the text\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "eval_dataset = train_test_split[\"test\"]\n",
        "\n",
        "from transformers import GPT2ForSequenceClassification\n",
        "\n",
        "# Define the number of labels (positive/negative)\n",
        "num_labels = 2\n",
        "\n",
        "# Load GPT-2 with a sequence classification head\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=num_labels)\n",
        "model.config.pad_token_id = tokenizer.pad_token_id # Set padding token id for the model\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "    # Define evaluation metric\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",  # Output directory\n",
        "        report_to=\"wandb\",\n",
        "        run_name=\"gpt2_classification_12_2\",  # Run name\n",
        "        num_train_epochs=2,  # Number of training epochs\n",
        "        per_device_train_batch_size=8,  # Batch size per device during training\n",
        "        per_device_eval_batch_size=8,   # Batch size for evaluation\n",
        "        warmup_steps=500,  # Number of warmup steps for learning rate scheduler\n",
        "        weight_decay=0.01,  # Strength of weight decay\n",
        "        logging_dir=\"./logs\",  # Directory for storing logs\n",
        "        logging_steps=1,\n",
        "        eval_strategy=\"epoch\",\n",
        "    )\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "        model=model,  # The model to train\n",
        "        args=training_args,  # The training arguments\n",
        "        train_dataset=train_dataset,  # The training dataset\n",
        "        eval_dataset=eval_dataset,  # The evaluation dataset\n",
        "        compute_metrics=compute_metrics, # The function to compute metrics\n",
        "    )\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "results = trainer.evaluate()\n",
        "print(f\"Validation Loss: {results['eval_loss']}\")\n",
        "\n",
        "model.save_pretrained('fine-tuned-gpt2')\n",
        "tokenizer.save_pretrained('fine-tuned-gpt2')\n"
      ],
      "metadata": {
        "id": "MeSowkCBIybh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}