# Purpose: Use this file to find the per-sample max norm/clip threshold for 
#          your dataset. Be sure to change the "self.max_norm" property in the 
#          "client_dp_fl.py" and "grpc_dp_server.py" files to match the one
#          generated by this script.
# Usage: python max_norm_calc.py --cid <client number> --batches [batch number]
# Example: python max_norm_calc.py 0 --batches 200
# Recommendation: For number batches it is best practice to use 10-20% of the 
#                 dataset. Run with no arguments for batches to run on 15% of
#                 on the dataset
# Note: This script computes per-sample L2 activation norms by flattening
#       all tokens and features into a single vector per sample.
#       The resulting norms should be used to set the per-sample clipping threshold
#       for differential privacy.

import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import torch
import numpy as np
import pickle
from torch.utils.data import DataLoader
from models.split_bert_model import BertSplitConfig, BertModel_Client
from tqdm import tqdm
import argparse

def collect_activation_norms(client_id, num_batches=None, batch_size=16, default_pct=0.15):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load dataset
    with open(f"client_data/client_sst2_{client_id}.pkl", "rb") as f:
        dataset = pickle.load(f)

    total_samples = len(dataset)
    if num_batches is None:
        num_batches = max(1, int((default_pct * total_samples) // batch_size))
        print(f"[Info] No --batches provided. Using {default_pct*100:.0f}% of dataset → {num_batches} batches")

    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Load model
    split_config = BertSplitConfig(split_layer=5)
    model = BertModel_Client(split_config).to(device)
    model.eval()

    norms = []

    # Collect Activation Norms
    with torch.no_grad():
        for i, batch in enumerate(tqdm(dataloader, desc=f"Collecting activation norms for client {client_id}")):
            if i >= num_batches:
                break

            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device).float()

            activations, _ = model(input_ids, attention_mask)
            batch_size = activations.size(0)
            # Per sample norm: flatten tokens and hidden dims
            batch_norms = activations.detach().reshape(batch_size, -1).norm(p=2, dim=1).cpu().numpy()
            norms.extend(batch_norms)

    norms = np.array(norms)
    median_norm = np.median(norms)
    min_norm = np.min(norms)
    max_norm = np.max(norms)
    std = np.std(norms)

    # Print result
    print("\n!Activation Norm Stats:")
    print(f"  ▸ Min:    {min_norm:.2f}")
    print(f"  ▸ Max:    {max_norm:.2f}")
    print(f"  ▸ Median: {median_norm:.2f}")

    print("\n!Recommendation:")
    print(f"  With activations having norms ~{median_norm:.0f}, you should set:")
    print(f"    clip_threshold ≈ {median_norm:.2f} (or up to {median_norm + std:.2f} to allow for variance, σ = {std:.2f}).However, research shows that the best max_norms are between 1 and 5.")
    print("  Then experiment with:")
    print("    noise_multiplier = 1.0 → decent privacy")
    print("    noise_multiplier = 2.0 → stronger privacy, more noise")
    print("    noise_multiplier = 0.5 → weaker privacy, better utility\n")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("client_id", type=int, help="Client ID")
    parser.add_argument("--batches", type=int, help="Number of batches to evaluate (optional)")
    args = parser.parse_args()

    collect_activation_norms(args.client_id, num_batches=args.batches)
